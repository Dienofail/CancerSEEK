# CancerSEEK Analysis Reproduction and Extension

This repository contains scripts to reproduce and extend the analyses presented in the Science paper "Detection and localization of surgically resectable cancers with a multi-analyte blood test" (Cohen et al., 2018), commonly known as the CancerSEEK paper.

## Scripts Description

Below is a description of the main Python (`.py`) and Shell (`.sh`) scripts in this repository:

*   `README.md`: This file, providing an overview and documentation for the repository.

*   `process_supplemental_data.py`: Loads, cleans, and preprocesses the supplemental data tables (S1-S10) provided with the original CancerSEEK publication. This is a crucial first step for preparing the data for any subsequent analysis. It handles tasks like cleaning column names, converting data types, and imputing missing values based on methods described or implied in the paper.

*   `plotting.py`: Contains a suite of functions for generating various plots relevant to the CancerSEEK analysis. This includes ROC curves, sensitivity plots (overall, by cancer subtype, by stage), tissue localization accuracy plots, and confusion matrices. It aims to replicate figures from the paper and provide visualization tools for extended analyses.

*   `models.py`: (Assumed location - not provided in context) This script likely houses the implementations of the machine learning models (e.g., Logistic Regression, XGBoost, Random Forest, TensorFlow models, Mixture of Experts) and the nested cross-validation framework used for training and evaluating both the detection (cancer vs. normal) and localization (cancer type prediction) tasks.

*   `paper_reproduction.py`: The core script for reproducing the main CancerSEEK results. It trains and evaluates the combined cancer detection and tissue localization pipeline described in the paper. It allows users to select different model types (e.g., the original Logistic Regression or alternatives like XGBoost), specify target specificities for evaluation, and optionally include additional features beyond the original 8 proteins + omega score for extension analyses.

*   `feature_subsample.py`: An extension script designed to investigate the impact of using *fewer* features than the original set. It uses feature importance scores (derived from the paper's Table S9 or potentially recalculated) to incrementally build models, starting with the most important feature and adding features one by one, evaluating performance at each step.

*   `feature_oversample.py`: An extension script that explores the effect of using *more* features than the original 8 (+ omega score). It starts with all available numeric features and iteratively removes the least important ones (based on Random Forest feature importance) to analyze how model performance changes as the feature set is reduced towards the original set.

*   `synthetic_feature_oversample.py`: An extension script that investigates the impact of adding *synthetic* features to the original feature set. These synthetic features are generated with a controlled target correlation (`alpha`) to the cancer status outcome while maintaining a specified degree of linear independence (`orthogonality`) from the existing features. This helps explore whether adding informative, but potentially uncorrelated, signals can improve model performance.

*   `plot_s10_confusion_matrix.py`: A utility script specifically designed to load the pre-computed confusion matrix data from the paper's Table S10 and generate a plot visually representing it, effectively replicating Figure 3B of the publication.

*   `compare_roc_curves.py`: A utility script that takes multiple saved ROC curve data files (generated by other scripts like `paper_reproduction.py` or the feature sub/oversampling scripts) and plots them together on a single graph. This is useful for visually comparing the performance of different models or feature sets.

*   `run_all_experiments.sh`: A shell script that automates the process of running the `paper_reproduction.py` script multiple times. It iterates through a predefined grid of parameters (e.g., different detection models, localization models, target specificities) and executes the main script for each combination, logging the output. This facilitates systematic exploration of different model configurations.

*   `run_synthetic_oversample.sh`: A specific shell script to execute the `synthetic_feature_oversample.py` experiment with a defined set of parameters (e.g., number of synthetic features to add, target alpha, orthogonality level, CV folds).
